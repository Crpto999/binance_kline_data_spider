# -*- coding: utf-8 -*-
import sys
import time
import zipfile
from glob import glob
from itertools import product
from pathlib import Path
import os

import numpy as np
import pandas as pd
from joblib import Parallel, delayed
from tqdm import tqdm
from config import *

pd.set_option('display.max_rows', 1000)
pd.set_option('expand_frame_repr', False)  # å½“åˆ—å¤ªå¤šæ—¶ä¸æ¢è¡Œ


# é€šè¿‡æ‰€æœ‰zipæ–‡ä»¶çš„æ–‡ä»¶åå‰ç¼€è·å–å¸ç§åç§°åˆ—è¡¨
def extract_coin_names(folder_path):
    zip_files = glob(os.path.join(folder_path, '*.zip'))
    print(f'å‘ç° {len(zip_files)} ä¸ª{mode}zip æ–‡ä»¶.')
    time.sleep(1)
    coin_names = set()  # ä½¿ç”¨é›†åˆæ¥é¿å…é‡å¤çš„å¸ç§åç§°
    for zip_file in zip_files:
        coin_name = os.path.basename(zip_file).split('-')[0]
        coin_names.add(coin_name)
    coin_names_list = list(coin_names)
    coin_names_list.sort()
    return coin_names_list


def all_merge_csv(folder_path):
    matching_files = list(Path(folder_path).glob(f"*{'_merge'}*.csv"))
    merge_files = set()  # ä½¿ç”¨é›†åˆæ¥é¿å…é‡å¤çš„å¸ç§åç§°

    # ç»Ÿè®¡å·²å­˜åœ¨å¤šå°‘ä¸ª _merge çš„æ–‡ä»¶
    existing_merge_files_count = len(matching_files)
    if existing_merge_files_count > 2:
        print(f"æ£€æŸ¥åˆ°ä¸Šæ¬¡æœ‰ä»»åŠ¡ä¸­æ–­ã€‚ä¸Šæ¬¡å·²å®Œæˆ {existing_merge_files_count} ä¸ªå¸ç§çš„æ¸…æ´—ä»»åŠ¡ï¼Œå¼€å§‹ç»­æ´—.")

        for merge_file in matching_files:
            coin_name = os.path.basename(merge_file).split('_')[0]
            merge_files.add(coin_name)

        merge_files_list = list(merge_files)
        merge_files_list.sort()

        # åˆ é™¤é™¤äº†å¸¦æœ‰ _merge åç¼€çš„æ–‡ä»¶ä¹‹å¤–çš„å…¶ä»– CSV æ–‡ä»¶
        for file in Path(folder_path).glob("*.csv"):
            if "_merge" not in file.name:
                file.unlink()

        return merge_files_list
    else:
        return []


# è§£å‹å¹¶åˆ é™¤zipæ–‡ä»¶
def unzip_and_delete_zip(zip_files, folder_path):
    # æŸ¥æ‰¾æŒ‡å®šå¸ç§çš„zipæ–‡ä»¶
    for zip_file in zip_files:
        with zipfile.ZipFile(zip_file, 'r') as zip_ref:
            zip_ref.extractall(folder_path)  # è§£å‹
        # åˆ é™¤åŸzipæ–‡ä»¶
        # os.remove(zip_file)
    # print(f"{coin_name}è§£å‹å®Œæˆ")


def process_single_file(file):
    # è¯»å–æ–‡ä»¶çš„ç¬¬ä¸€è¡Œ
    with open(file, 'r') as f:
        first_line = f.readline().strip()  # .strip() ç§»é™¤å¯èƒ½çš„å‰åç©ºç™½å­—ç¬¦

    # æ£€æŸ¥ç¬¬ä¸€è¡Œæ˜¯å¦åŒ…å«ä»»ä½•é¢„æœŸçš„åˆ—å
    has_header = any(col_name in first_line for col_name in ["open_time", "open", "high", "low", "close"])

    # æ ¹æ®æ–‡ä»¶æ˜¯å¦æœ‰åˆ—åæ¥è¯»å–æ•°æ®
    df = pd.read_csv(file, header=0 if has_header else None)

    # å¸å®‰APIè¿”å›çš„éƒ¨åˆ†æ–‡ä»¶æ²¡æœ‰åˆ—åï¼Œå¦‚æœæ²¡æœ‰åˆ—åï¼Œéœ€è¦æ‰‹åŠ¨æŒ‡å®š
    if not has_header:
        df.columns = [
            "open_time", "open", "high", "low", "close", "volume",
            "close_time", "quote_volume", "count",
            "taker_buy_volume", "taker_buy_quote_volume", "ignore"
        ]
    # å°†åˆ—åæ˜ å°„åˆ°æ–°çš„åˆ—å
    column_mapping = {
        "open_time": "candle_begin_time",
        "count": "trade_num",
        "taker_buy_volume": "taker_buy_base_asset_volume",
        "taker_buy_quote_volume": "taker_buy_quote_asset_volume"
    }
    df.rename(columns=column_mapping, inplace=True)

    # æ·»åŠ æ–°çš„åˆ—
    df['symbol'] = file.split(os.sep)[-1].split('USDT')[0] + '-USDT'
    # æ³¨æ„ï¼šavg_price_1m å’Œ avg_price_5m éœ€è¦åç»­è®¡ç®—

    # è½¬æ¢æ—¶é—´æ ¼å¼
    df['candle_begin_time'] = pd.to_datetime(df['candle_begin_time'], unit='ms')

    # åˆ é™¤ä¸éœ€è¦çš„åˆ—
    df.drop(['close_time', 'ignore'], axis=1, inplace=True)

    return df


def process_coin_files(files):
    dataframes = Parallel(n_jobs=max(os.cpu_count() - 1, 1))(
        delayed(process_single_file)(file) for file in files
    )
    merged_df = pd.concat(dataframes)

    merged_df.sort_values(by='candle_begin_time', inplace=True)
    merged_df.drop_duplicates(subset=['candle_begin_time'], inplace=True, keep='last')
    merged_df.reset_index(drop=True, inplace=True)
    # å¡«å……ç©ºç¼ºçš„æ•°æ®
    start_date = merged_df.iloc[0]['candle_begin_time']
    end_date = merged_df.iloc[-1]['candle_begin_time']
    benchmark = pd.DataFrame(pd.date_range(start=start_date, end=end_date, freq='1min'))  # ä¿®æ”¹é¢‘ç‡åˆ«å
    benchmark.rename(columns={0: 'candle_begin_time'}, inplace=True)
    merged_df = pd.merge(left=benchmark, right=merged_df, on='candle_begin_time', how='left', sort=True, indicator=True)
    merged_df['close'] = merged_df['close'].ffill()  # ä½¿ç”¨æ¨èçš„æ–¹æ³•
    merged_df['symbol'] = merged_df['symbol'].ffill()  # ä½¿ç”¨æ¨èçš„æ–¹æ³•
    for column in ['open', 'high', 'low']:
        merged_df[column] = merged_df[column].fillna(merged_df['close'])
    _ = ['volume', 'quote_volume', 'trade_num', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume']
    merged_df[_] = merged_df[_].fillna(0)
    # merged_df = merged_df[merged_df['_merge'] == 'left_only']

    # å°†ç´¢å¼•è½¬æ¢ä¸ºDatetimeIndexï¼Œå¦‚æœå®ƒè¿˜ä¸æ˜¯
    merged_df.set_index('candle_begin_time', inplace=True)

    merged_df['avg_price_1m'] = merged_df['quote_volume'] / merged_df['volume']
    merged_df['avg_price_5m'] = merged_df['quote_volume'].rolling(window=5).sum() / merged_df['volume'].rolling(
        window=5).sum()
    merged_df['avg_price_5m'] = merged_df['avg_price_5m'].shift(-4)
    merged_df['avg_price_1m'] = merged_df['avg_price_1m'].fillna(merged_df['open'])  # ç›´æ¥èµ‹å€¼
    merged_df['avg_price_5m'] = merged_df['avg_price_5m'].fillna(merged_df['open'])  # ç›´æ¥èµ‹å€¼

    hourly_df = merged_df.resample('1h').agg({  # ä¿®æ”¹é¢‘ç‡åˆ«å
        'open': 'first',
        'high': 'max',
        'low': 'min',
        'close': 'last',
        'volume': 'sum',
        'quote_volume': 'sum',
        'trade_num': 'sum',
        'taker_buy_base_asset_volume': 'sum',
        'taker_buy_quote_asset_volume': 'sum',
        'symbol': 'first',
        'avg_price_1m': 'first',
        'avg_price_5m': 'first',
    })
    hourly_df.reset_index(inplace=True)

    return hourly_df


def process_single_combination(df, stop_profit, stop_loss):
    from numpy.lib.stride_tricks import sliding_window_view
    """
    è®¡ç®—ç‰¹å®šæ­¢ç›ˆå’Œæ­¢æŸç­–ç•¥ä¸‹çš„è§¦å‘æƒ…å†µã€‚

    :param df: pandas DataFrameï¼ŒåŒ…å«å¿…è¦çš„åˆ— ['candle_begin_time', 'high', 'low', 'avg_price_1m', 'open']
    :param stop_profit: floatï¼Œæ­¢ç›ˆç™¾åˆ†æ¯”ï¼ˆä¾‹å¦‚ 0.05 è¡¨ç¤º 5%ï¼‰
    :param stop_loss: floatï¼Œæ­¢æŸç™¾åˆ†æ¯”ï¼ˆä¾‹å¦‚ -0.03 è¡¨ç¤º 3%ï¼‰
    :return: pandas Seriesï¼Œè¡¨ç¤ºæ¯ä¸ªæ—¶é—´ç‚¹çš„æ­¢ç›ˆæ­¢æŸè§¦å‘æƒ…å†µ
             0 - æœªè§¦å‘
            -1 - æ­¢æŸå…ˆè§¦å‘
             1 - æ­¢ç›ˆå…ˆè§¦å‘
             2 - åŒæ—¶è§¦å‘
    """
    # ç¡®ä¿ DataFrame æŒ‰æ—¶é—´æ’åºï¼Œå¹¶é‡ç½®ç´¢å¼•
    df = df.sort_values('candle_begin_time').reset_index(drop=True)

    # æå–å¿…è¦çš„åˆ—ä¸º NumPy æ•°ç»„ï¼Œä¾¿äºé«˜æ•ˆè®¡ç®—
    high = df['high'].values
    low = df['low'].values
    avg_price_1m = df['avg_price_1m'].values
    n = len(df)  # æ•°æ®æ€»è¡Œæ•°

    # è®¡ç®—æ¯ä¸ªæ—¶é—´ç‚¹çš„æ­¢ç›ˆä»·å’Œæ­¢æŸä»·
    stop_profit_price = avg_price_1m * (1 + stop_profit)
    stop_loss_price = avg_price_1m * (1 + stop_loss)

    # å®šä¹‰æ—¶é—´çª—å£å¤§å°ä¸º24
    window_size = 24

    # å¡«å…… high å’Œ low æ•°æ®ï¼Œä»¥ç¡®ä¿æ‰€æœ‰çª—å£éƒ½æœ‰å®Œæ•´çš„æ•°æ®
    # ä½¿ç”¨ 'edge' æ¨¡å¼ï¼Œå³ä½¿ç”¨æœ€åä¸€ä¸ªæœ‰æ•ˆå€¼è¿›è¡Œå¡«å……
    pad_width = window_size
    high_padded = np.pad(high, (0, pad_width), mode='edge')  # å½¢çŠ¶ (n + window_size,)
    low_padded = np.pad(low, (0, pad_width), mode='edge')  # å½¢çŠ¶ (n + window_size,)

    # ä½¿ç”¨ sliding_window_view æ„å»ºæ»‘åŠ¨çª—å£
    # æ¯ä¸ªçª—å£åŒ…å«24ä¸ªå…ƒç´ ï¼Œä»å½“å‰è¡Œå¼€å§‹
    high_windows = sliding_window_view(high_padded, window_shape=window_size)[:n]  # å½¢çŠ¶ (n, window_size)
    low_windows = sliding_window_view(low_padded, window_shape=window_size)[:n]  # å½¢çŠ¶ (n, window_size)

    # ç”Ÿæˆ final_price
    # å¯¹ avg_price_1m è¿›è¡Œå¡«å……ï¼Œä»¥è·å–æ¯ä¸ªçª—å£åçš„ç¬¬24ä¸ªä»·æ ¼
    avg_price_padded = np.pad(avg_price_1m, (0, pad_width + 1), mode='edge')  # å½¢çŠ¶ (n + window_size +1,)
    final_price = avg_price_padded[window_size: window_size + n]  # å½¢çŠ¶ (n,)

    # å°† final_price æ·»åŠ ä¸ºæ¯ä¸ªçª—å£çš„æœ€åä¸€åˆ—
    # final_price[:, np.newaxis] å°†å…¶å½¢çŠ¶ä» (n,) è½¬æ¢ä¸º (n, 1) ä»¥ä¾¿äºæ‹¼æ¥
    high_full = np.concatenate([high_windows, final_price[:, np.newaxis]], axis=1)  # å½¢çŠ¶ (n, window_size + 1)
    low_full = np.concatenate([low_windows, final_price[:, np.newaxis]], axis=1)  # å½¢çŠ¶ (n, window_size + 1)

    # ------------------ è§¦å‘æ£€æµ‹é€»è¾‘å¼€å§‹ ------------------
    # åŒ…å« final_price åœ¨å†…çš„è§¦å‘æ£€æµ‹
    profit_trigger = high_full > stop_profit_price[:, np.newaxis]  # å½¢çŠ¶ (n, window_size +1)
    loss_trigger = low_full < stop_loss_price[:, np.newaxis]  # å½¢çŠ¶ (n, window_size +1)

    # å¯¹äºæ¯ä¸€è¡Œï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªè§¦å‘çš„ä½ç½®
    profit_any = np.any(profit_trigger, axis=1)
    profit_indices = np.argmax(profit_trigger, axis=1)
    profit_indices = np.where(profit_any, profit_indices, float('inf'))

    loss_any = np.any(loss_trigger, axis=1)
    loss_indices = np.argmax(loss_trigger, axis=1)
    loss_indices = np.where(loss_any, loss_indices, float('inf'))

    # è®¡ç®—æœ€ç»ˆçš„æ­¢ç›ˆæ­¢æŸç»“æœ
    result = np.zeros(n, dtype=int)

    # æ­¢æŸå…ˆè§¦å‘
    mask_loss_first = (loss_indices < profit_indices)
    result[mask_loss_first] = -1

    # æ­¢ç›ˆå…ˆè§¦å‘
    mask_profit_first = (profit_indices < loss_indices)
    result[mask_profit_first] = 1

    # åŒæ—¶è§¦å‘ï¼ˆåœ¨åŒä¸€æ—¶é—´ç‚¹ï¼‰
    mask_both_same = (profit_indices == loss_indices) & (profit_indices != float('inf'))
    result[mask_both_same] = 2

    # å°†ç»“æœè½¬æ¢ä¸º pandas Series
    return pd.Series(result, index=df.index, name=f'stop[{stop_profit}_{stop_loss}]')


def process_stop(df, stop_loss_list, stop_profit_list, calc_stop=True, use_parallel=False, n_jobs=-1):
    if not calc_stop:
        return df

    # Generate all combinations of stop profit and stop loss
    stop_all_combinations = list(product(stop_profit_list, stop_loss_list))

    if use_parallel:
        # Parallel processing
        results_dfs = Parallel(n_jobs=n_jobs)(
            delayed(process_single_combination)(df, stop_profit, stop_loss)
            for stop_profit, stop_loss in stop_all_combinations
        )
    else:
        # Serial processing
        results_dfs = []
        for stop_profit, stop_loss in stop_all_combinations:
            result_df = process_single_combination(df, stop_profit, stop_loss)
            results_dfs.append(result_df)

    # Concatenate all the result DataFrames along the columns
    results_combined = pd.concat(results_dfs, axis=1)

    # Merge the original DataFrame with the results
    df_final = pd.concat([df, results_combined], axis=1)

    return df_final


def get_merge_csv_files(folder_path):
    csv_files = glob(os.path.join(folder_path, '*.csv'))

    grouped_files = {}
    for file in csv_files:

        # if '_merge' in file:
        if '_merged' in os.path.basename(file):
            continue
        coin_name = os.path.basename(file).split('-')[0]

        grouped_files.setdefault(coin_name, []).append(file)

    for coin_name, files in grouped_files.items():
        try:
            hourly_df = process_coin_files(files)

            df_final = process_stop(hourly_df, stop_loss_list, stop_profit_list)

            df_final.to_csv(os.path.join(folder_path, f'{coin_name}_merged.csv'), index=False)

        except Exception as exc:
            print(f"\n {coin_name}ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {exc}")


# åˆ é™¤æœªåˆå¹¶çš„CSVæ–‡ä»¶
def delete_unmerged_csv_files(folder_path):
    # æŸ¥æ‰¾æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰çš„CSVæ–‡ä»¶
    csv_files = glob(os.path.join(folder_path, '*.csv'))

    for csv_file in csv_files:
        # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦ä¸åŒ…å« '_merged'
        if '_merged' not in os.path.basename(csv_file):
            os.remove(csv_file)  # åˆ é™¤æ–‡ä»¶


if __name__ == "__main__":
    # é»˜è®¤å€¼
    target = 'swap'
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        target = sys.argv[1]
    if target == "spot":
        download_directory = ç°è´§ä¸´æ—¶ä¸‹è½½æ–‡ä»¶å¤¹
        mode = "ç°è´§"
    else:
        download_directory = æ°¸ç»­åˆçº¦ä¸´æ—¶ä¸‹è½½æ–‡ä»¶å¤¹
        mode = "åˆçº¦"

    coins_to_clean = extract_coin_names(download_directory)
    merge_csvs = all_merge_csv(download_directory)

    # å¦‚æœä»»åŠ¡ä¸­æ–­ï¼Œè¯†åˆ«æ–­ç‚¹ï¼Œç»§ç»­æ¸…ç†
    if len(merge_csvs) >= 2:
        index_next_clean = coins_to_clean.index(merge_csvs[-2])
        coins_to_clean = coins_to_clean[index_next_clean:]

    with tqdm(total=len(coins_to_clean), desc="æ€»ä½“è¿›åº¦", unit=mode) as pbar:
        for coin_name in coins_to_clean:
            coin = coin_name.replace("USDT", "-USDT")
            # æ­¥éª¤1: è§£å‹
            zip_files = glob(os.path.join(download_directory, f'{coin_name}*.zip'))
            file_num = len(zip_files)
            pbar.set_description(f"ğŸ“¦ æ­£åœ¨è§£å‹{file_num}ä¸ª{coin}çš„zipæ–‡ä»¶")
            unzip_and_delete_zip(zip_files, download_directory)  # è§£å‹æŒ‡å®šå¸ç§çš„zipæ–‡ä»¶å¹¶åˆ é™¤

            # æ­¥éª¤2: æ¸…æ´—åˆå¹¶
            pbar.set_description(f"ğŸš¿ æ­£åœ¨æ¸…æ´—åˆå¹¶{file_num}ä¸ª{coin}çš„csvæ–‡ä»¶")
            get_merge_csv_files(download_directory)

            # æ­¥éª¤3: åˆ é™¤è¿™ä¸ªå¸ç§çš„ä¸€åˆ†é’ŸCSV,å®Œæˆå¤„ç†
            delete_unmerged_csv_files(download_directory)
            pbar.update(1)
            pbar.set_description(f"ğŸ’› {file_num}ä¸ª{coin} çš„{mode}csvï¸æ¸…æ´—å®Œæˆï¼Œå·²åˆå¹¶ä¿å­˜")
            time.sleep(1)
    pbar.close()
